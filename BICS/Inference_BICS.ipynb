{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web_version",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKr52Kmj6E6k",
        "outputId": "be044375-8c92-4976-bbc9-78914cbac8da"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "os.chdir(\"/content/drive/My Drive/BOAZ Conference/final_result\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qvxf7H9PC3o",
        "outputId": "0c02dd12-c295-405b-fdd8-6b35d2a00ef7"
      },
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install umap-learn\n",
        "!pip install kss"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/fd/8a81047bbd9fa134a3f27e12937d2a487bd49d353a038916a5d7ed4e5543/sentence-transformers-2.0.0.tar.gz (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.1MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 28.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 21.8MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading https://files.pythonhosted.org/packages/35/03/071adc023c0a7e540cf4652fa9cad13ab32e6ae469bf0cc0262045244812/huggingface_hub-0.0.13-py3-none-any.whl\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 52.3MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 25.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (20.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.4.1)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-cp37-none-any.whl size=126711 sha256=1938456a9120e3ecec6a88759b6a82eed22e352188672b6302f49146e79a8b25\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/d2/98/d191289a877a34c68aa67e05179521e060f96394a3e9336be6\n",
            "Successfully built sentence-transformers\n",
            "\u001b[31mERROR: transformers 4.8.2 has requirement huggingface-hub==0.0.12, but you'll have huggingface-hub 0.0.13 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.0.13 sacremoses-0.0.45 sentence-transformers-2.0.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.8.2\n",
            "Collecting umap-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/69/85e7f950bb75792ad5d666d86c5f3e62eedbb942848e7e3126513af9999c/umap-learn-0.5.1.tar.gz (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/65/8189298dd3a05bbad716ee8e249764ff8800e365d8dc652ad2192ca01b4a/pynndescent-0.5.2.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.0.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.1-cp37-none-any.whl size=76569 sha256=ecaacf119d5a569db92d3948ec60f7c6e13d8625b28a3203b2ba4a636492ca39\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/df/d5/a3691296ff779f25cd1cf415a3af954b987fb53111e3392cf4\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.2-cp37-none-any.whl size=51362 sha256=78ef36143f79b2f327dcc881ca0d1c50abca04ba1e4a0037f9305ff7b82cc55d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/52/4e/4c28d04d144a28f89e2575fb63628df6e6d49b56c5ddd0c74e\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.2 umap-learn-0.5.1\n",
            "Collecting kss\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/ea/3030770642a58a08777dfa324a1b65a2f53f1574de8dd84424851f0c2ec7/kss-2.5.1-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.8MB/s \n",
            "\u001b[?25hInstalling collected packages: kss\n",
            "Successfully installed kss-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmeY4-sbMSVw"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import itertools"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Oif41I2J3y8"
      },
      "source": [
        "with open('umap_dim.pkl','rb') as f:\n",
        "  umap = pickle.load(f)\n",
        "\n",
        "total_df = pd.read_pickle('fined_cm_rocket_demoday.pkl')\n",
        "umap_embeddings = np.load('fined_sbert_umap_vec.npy')\n",
        "model_name = 'sent_bert'\n",
        "emb_model = SentenceTransformer(model_name)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6HTgiVFPUpg",
        "outputId": "cf387eda-0242-4e82-8e89-235b52e453f3"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import kss\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "eng_stopwords = stopwords.words('english')\n",
        "eng_stopwords = [word for word in eng_stopwords if len(word) > 1]\n",
        "eng_stopwords = [r'\\b'+word.replace(\"'\", \"\\b'\")+r'\\b' for word in eng_stopwords]\n",
        "\n",
        "stop_words = ['2020년 12월', '전년동기', '전년대비', '연결기준', '대비', '매출액', '영업이익', '당기순이익', \n",
        "                '별도기준', '당기', '경영실적' ,'전년동기', '연결대상', '종속회사', '전년', '동사', '흑자전환', '적자전환',\n",
        "               '성장', '국내', '매출', '기반', '주요', '증가', '영위'  '진행', '지속', '종속',\n",
        "              '관련', '상장', '개발',  '악화', '축소', '사람', '세상', '가치', '생각', '판매', '부진',\n",
        "              '사회', '변화', '목표', '경험', '정보', '혁신', '의미', '소통', '행복', '아이디어', '노력', \n",
        "              '시작', '문화', '시간', '시작', '준비', '한국', '인재', '기획', '선정', '전문가', '이상', '영업', '확대',\n",
        "              '수익', '영향', '개선', '코로나','목적', '하락', '실적', '부담', '확대', '코스닥', '감소', \n",
        "              '손실', '세계', '최고', '자신', '필요', '고민', '문제', '지원', '관리', '동기', '유지', '글로벌', '세계', \n",
        "              '보유', '사랑', '신규', '사원수', '자본금', '규모', '위치', '서울', '중소', '강남구', '마포구', '강소', \n",
        "              '역삼동', '구로구', '벤처', '성동구', '서초구', '노동부', '빌딩', '가산동', '고용', '송파구', '금천구']\n",
        "stop_words += eng_stopwords\n",
        "remove_words = \"(\"+\"|\".join(stop_words)+\")\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jBGMR3bKBiy"
      },
      "source": [
        "class document_embedding():\n",
        "  def __init__(self, text, remove_words):\n",
        "\n",
        "    self.text = text\n",
        "    self.remove_words = remove_words\n",
        "\n",
        "  def preprocessing(self, x):\n",
        "    assert len(x) > 10, '10글자 이상 입력하세요.' \n",
        "    text = re.sub('안녕하세요\\S*', '', x)\n",
        "    text = re.sub('\\S+[.]*%\\s*\\S+', '', text)\n",
        "    text = re.sub('\\d+년\\w*\\s*|\\d+월\\w*\\s*|\\d+일\\w*\\s*', '', text)\n",
        "    text = re.sub('[-=+,#/\\?:^$@*\\\"※~&%ㆍ!』/\\|\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', ' ', text)\n",
        "    text = re.sub('[\\W|0-9]+\\s{2,}','', text)\n",
        "    text = re.sub(r'\\\\[[^]]*\\\\]' ,'', text)\n",
        "    text = re.sub('【[^】]*】', '', text)\n",
        "    text = re.sub('●\\s+[A-Z|a-z]+','' , text)\n",
        "    text = re.sub(\"(http|ftp|https)://(?:[-\\w.]|(?:\\da-fA-F]{2}))+\\S+\", '' , text)\n",
        "    text = re.sub('\\w+@\\S+', '', text)\n",
        "    text = re.sub('\\xa0', ' ', text)\n",
        "    text = re.sub('[^가-힣|a-zA-Z|\\s|0-9|.]+', '', text)\n",
        "    text = re.sub('\\d[.]\\s*', '', text)\n",
        "    text = re.sub(r'(www[.]).+([.]com)', '', text)\n",
        "    text = re.sub('(www[.])\\S*', '', text)\n",
        "    text_list = kss.split_sentences(text)\n",
        "    text_list = list(map(lambda x: re.sub('(\\w*\\s*)'+remove_words+'(\\w*\\s*)', '', x), text_list))\n",
        "    text_list = list(map(lambda x: re.sub('\\s{2,}', ' ', x), text_list))\n",
        "    text_list = list(map(lambda x: x.strip(), text_list))\n",
        "    text_list = ' '.join(text_list)\n",
        "    return text_list\n",
        "\n",
        "  def get_emb_reduction(self, emb_model, reduction_model):\n",
        "    fined_text = self.preprocessing(self.text)\n",
        "    text_vec = emb_model.encode(fined_text).reshape(1,-1)\n",
        "    umap_vec = reduction_model.transform(text_vec)\n",
        "    return umap_vec"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRy_Og-eLKss"
      },
      "source": [
        "def identify_tokenizer(text):\n",
        "  return text\n",
        "\n",
        "\n",
        "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
        "    count = CountVectorizer(ngram_range=ngram_range, tokenizer=identify_tokenizer, lowercase=False).fit(documents)\n",
        "    t = count.transform(documents).toarray()\n",
        "    w = t.sum(axis=1)\n",
        "    tf = np.divide(t.T, w)\n",
        "    sum_t = t.sum(axis=0)\n",
        "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
        "    tf_idf = np.multiply(tf, idf)\n",
        "\n",
        "    return tf_idf, count\n",
        "\n",
        "def extract_top_n_words_per_topic(tf_idf, count, df,n=20):\n",
        "    words = count.get_feature_names()\n",
        "    labels = list(df.label)\n",
        "    tf_idf_transposed = tf_idf.T\n",
        "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
        "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
        "    return top_n_words\n",
        "\n",
        "def extract_topic_sizes(df):\n",
        "    topic_sizes = (df.groupby(['label'])\n",
        "                     .fined_token\n",
        "                     .count()\n",
        "                     .reset_index()\n",
        "                     .rename({\"label\": \"Topic\", \"fined_token\": \"Size\"}, axis='columns')\n",
        "                     .sort_values(\"Size\", ascending=False))\n",
        "    return topic_sizes\n",
        "\n",
        "def get_cluster_words(df, X, cluster_num):\n",
        "  agg_model = AgglomerativeClustering(n_clusters=cluster_num).fit(X)\n",
        "  pred = agg_model.labels_\n",
        "  if len(pred) > len(df):\n",
        "    df['label'] = pred[:-1]\n",
        "  else:\n",
        "    df['label'] = pred\n",
        "\n",
        "  cal_score = calinski_harabasz_score(X, pred)\n",
        "  sil_score = silhouette_score(X, pred, metric='cosine')\n",
        "  print('cal_score :{0:0.4f} sil_score : {1:0.4f}'.format(cal_score, sil_score))\n",
        "\n",
        "  doc_per_cls = df.groupby(['label'], as_index=False).agg({'fined_text': 'sum','fined_token': 'sum'})\n",
        "  tf_idf, count = c_tf_idf(doc_per_cls.fined_token.values, m=len(df))\n",
        "  top_n_words = extract_top_n_words_per_topic(tf_idf, count, doc_per_cls, n=20)\n",
        "  topic_sizes = extract_topic_sizes(df)\n",
        "\n",
        "  \n",
        "  key = pred[-1]\n",
        "  value = [word[0] for word in top_n_words[key]]\n",
        "  print(key, ':', value)\n",
        "\n",
        "def get_neighbors(df, doc_vec):\n",
        "  agg_model = AgglomerativeClustering(n_clusters=None, distance_threshold=0.5).fit(doc_vec)\n",
        "  key = agg_model.labels_[-1]\n",
        "  ind = agg_model.labels_ == key\n",
        "  result = total_df.loc[ind[:-1], ['company_name', 'train_data']]\n",
        "  return result"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30cBypc-W2dY",
        "outputId": "e4870a95-c709-4ef6-c740-ade3a7d04192"
      },
      "source": [
        "# input에 텍스트 넣어주면\n",
        "# 1) 38개의 산업 분류 중 입력된 설명에 해당하는 Label 반환 & Label 키워드 반환\n",
        "# 2) 입력된 설명과 유사한 기업들로 구성된 DataFrame 반환 (neighbor_df)\n",
        "\n",
        "test_text = input()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CJ그룹의 디저트 카페 프랜차이즈 기업이다. 2002년 신촌점을 시작으로 2018년 현재 전국에 1,000여개의 점포를 운영하고 있다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO7HOP76xbp4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "7c7a7b18-ccc8-40a7-e053-07c6cd758ab5"
      },
      "source": [
        "new_doc = document_embedding(test_text, remove_words)\n",
        "new_doc_vec = new_doc.get_emb_reduction(emb_model, umap)\n",
        "total_vec = np.concatenate([umap_embeddings, new_doc_vec], axis=0)\n",
        "get_cluster_words(total_df, total_vec, 38)\n",
        "neighbor_df = get_neighbors(total_df, total_vec).rename(columns={'company_name':'회사명', 'train_data':'기업 설명'}).reset_index(drop=True)\n",
        "neighbor_df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: FutureWarning: Possible nested set at position 3\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cal_score :6785.9234 sil_score : 0.6307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 : ['식품', '커피', '푸드', '음식', '유통', '소비자', '건강', '농업', '가공', '배달', '사료', '외식', '브랜드', '온라인', '생산', '카페', '매장', '주문', '음료', '맛집']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>회사명</th>\n",
              "      <th>기업 설명</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>벤디스</td>\n",
              "      <td>벤디스 대표 조정호 www.VENDYS.co.kr 는 설립된 기업 O2O Onlin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>리치빔</td>\n",
              "      <td>주 리치빔은 설립하여 견고하고 실질적인 연구와 지식을 바탕으로 끊임없는 새로운 지식...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>란체스터에프앤비</td>\n",
              "      <td>란체스터에프앤비는 싸움의고수 처음으로 1인 보쌈의 시대를 열어 현재 190개의 가맹...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>얌테이블</td>\n",
              "      <td>수산신선식품 온라인커머스 1등 스타트업얌테이블은 거제 산지에 FPC Fish Pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>본아이에프</td>\n",
              "      <td>본그룹 본아이에프 본푸드서비스 순수본 본에프디 대학로의 작은 본죽 1호점 만에 한식...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        회사명                                              기업 설명\n",
              "0       벤디스  벤디스 대표 조정호 www.VENDYS.co.kr 는 설립된 기업 O2O Onlin...\n",
              "1       리치빔  주 리치빔은 설립하여 견고하고 실질적인 연구와 지식을 바탕으로 끊임없는 새로운 지식...\n",
              "2  란체스터에프앤비  란체스터에프앤비는 싸움의고수 처음으로 1인 보쌈의 시대를 열어 현재 190개의 가맹...\n",
              "3      얌테이블  수산신선식품 온라인커머스 1등 스타트업얌테이블은 거제 산지에 FPC Fish Pro...\n",
              "4     본아이에프  본그룹 본아이에프 본푸드서비스 순수본 본에프디 대학로의 작은 본죽 1호점 만에 한식..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLyzXTqbY-4-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}